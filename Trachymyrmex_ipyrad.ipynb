{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Thesis Pipeline\n",
    "This pipeline will be modeled after:\n",
    "1. [Carol Rowe's Allenrolfea analysis](https://digitalcommons.usu.edu/all_datasets/39/)\n",
    "2. [emprical ipyrad API pedicularis](https://nbviewer.jupyter.org/github/dereneaton/ipyrad/blob/master/tests/cookbook-empirical-API-1-pedicularis.ipynb)\n",
    "3. see Grundwald Lab [Poppr analysis](http://grunwaldlab.github.io/Population_Genetics_in_R/index.html) tutorial on github\n",
    "<br>\n",
    "<br>\n",
    "*to be updated with further population analysis as well* <br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ~!~ important ~!~ \n",
    "#### Reconnecting to a notebook after a job has ended\n",
    "Use this after a job has finished and you are reaccessing a notebook! After your session is over, or you copy and paste a new notebook with old notes/data ect. the IPYNB does not know the pathways of the json files (what the ipyrad uses to build assemblies). You need to tell it to load them with this command:<br>\n",
    "`ipyrad.load.load_json(\"<your_assembly>.json\")`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#required modules/programs/packages\n",
    "#seaborn\n",
    "#pandas\n",
    "#numpy\n",
    "#ipyparallel\n",
    "#ipyrad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Check `ipcluster` instance with a profile**<br>\n",
    "First we need to check our paralization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ipyparallel as ipp\n",
    "print 'ipyparallel version is', ipp.__version__\n",
    "mpi1 = ipp.Client(profile=\"MPI2019_06_21\")\n",
    "print 'MPI2019_06_21 has',len(mpi1), 'cores'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *ipyrad*\n",
    "The only library we need to import right now is *ipyrad*. Printing the version number of *ipyrad* is good practice to keep a record of which software version we are using. \n",
    "<br>\n",
    "This markdown follows the [*ipyrad* API user guide](https://ipyrad.readthedocs.io/API_user-guide.html).<br>\n",
    "See: * Eaton, D. A. R., & Overcast, I. (2019). ipyrad: interactive assembly and analysis of RADseq data sets. In prep. *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#requires ipyrad\n",
    "import ipyrad as ip\n",
    "print ip.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IPYRAD FIRST STEPS\n",
    "We first used FastQC to examine our files and check our illumina sequences. The sequences looked good overall, and had expected phred scores, with no noticble issue with adaptoers. We then ran step 1, and then merged our assemblys and removed samples with less than 0.5 million reads (see next cell). We have made a directory with all the demultiplexed reads after step 1. This is where we will call our files in our parameters: <br>\n",
    "`<assembly_name>.set_params('sorted_fastq_path', '/fs/project/adams.1970/cardenas.61/2019_analysis/reduced_samples/*.fastq.gz')`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## reduced data assembly\n",
    "fulldata.stats is the whole dataset, a new data set with all samples with > 0.05 million reads will go into a new directory labled `reduced_samples`<br>\n",
    "**removed samples:**<br>\n",
    "`Metro_Park_CC184\t1\t15963`<br>\n",
    "`Metro_Park_CC185\t1\t19655`<br>\n",
    "`Metro_Park_CC187\t1\t16969`<br>\n",
    "`Metro_Park_CC252\t1\t42792`<br>\n",
    "`Metro_Park_CC253\t1\t34129`<br>\n",
    "*There are other samples w/ <1.0 million but for now lets keep it simple. We do not want to over filter our data.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reduced_data = ip.Assembly(\"reduced_data\")\n",
    "reduced_data.get_params() # need to change these parameters!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameters 1\n",
    "Here we will set our generic parameters for our primary assembly, we run all the way through with this analysis after creating a branch step that we can use to make minor adjustments.<br>\n",
    "For example: we will branch after step one; (branch1) and from that branch we will create new assembly branches (branch1 tells reduced2 to copy branch1's .json file). <br><br>\n",
    "* The only filter we are keeping consistent is we want reads that are bigger than 50bp, no small fragments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "reduced_data.set_params('project_dir', '/fs/project/adams.1970/cardenas.61/2019_analysis') \n",
    "reduced_data.set_params('sorted_fastq_path', '/fs/project/adams.1970/cardenas.61/2019_analysis/reduced_samples/*.fastq.gz')\n",
    "reduced_data.set_params('assembly_method', 'denovo+reference')\n",
    "reduced_data.set_params('datatype', 'pairddrad')\n",
    "reduced_data.set_params('reference_sequence', './Tzet_genomic.fna')\n",
    "reduced_data.set_params('restriction_overhang', 'TGCAG, CGG')\n",
    "reduced_data.set_params('max_low_qual_bases', '5')\n",
    "reduced_data.set_params('mindepth_statistical', '6')\n",
    "reduced_data.set_params('mindepth_majrule', '6')\n",
    "reduced_data.set_params('clust_threshold', '0.85')\n",
    "reduced_data.set_params('filter_adapters', '1')\n",
    "reduced_data.set_params('filter_min_trim_len', '50')\n",
    "reduced_data.set_params('max_Hs_consens','8,8')\n",
    "reduced_data.set_params('min_samples_locus', '4')\n",
    "reduced_data.set_params('max_SNPs_locus', '20, 30')\n",
    "reduced_data.set_params('trim_reads', '0, 0, 0, 0')\n",
    "reduced_data.set_params('output_formats', '*')\n",
    "reduced_data.get_params()\n",
    "\n",
    "#.set_params('', '')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "reduced_data.run(\"1\",ipyclient=mpi1)\n",
    "reduced_data.stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "branch1 = reduced_data.branch(\"branch1\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "now we will just run through with the mostly default parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reduced_data.run(\"234567\",ipyclient=mpi1)\n",
    "reduced_data.stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reduced_data.stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! cat /fs/project/adams.1970/cardenas.61/2019_analysis/reduced_data_outfiles/reduced_data_stats.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimizing parameters\n",
    "We are going to see what parameters give us the least data loss, we **do not want to over filter our data**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step two and three branches\n",
    "First see how parameter `filter_adapters` set at = 2 compares to the primary assembly<br>\n",
    "Now see how parameter `trim_reads` set at = '0, 140, 0, 135'; `(R1>,<R1,R2>,<R2)`  compares to the primary assembly<br>\n",
    "Then see how adjusting `clust_threshold` set at = 0.80, and 0.90 compares to the 0.85 value in the primary assembly. <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "filter2 = branch1.branch(\"filter2\")\n",
    "trimread = branch1.branch(\"trimread\")\n",
    "clust80 = branch1.branch(\"clust80\")\n",
    "clust90 = branch1.branch(\"clust90\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "filter2.set_params('filter_adapters', '2')\n",
    "trimread.set_params('trim_reads', '0, 140, 0, 135')\n",
    "clust80.set_params('clust_threshold', '0.85')\n",
    "clust90.set_params('clust_threshold', '0.90')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "filter2.run(\"2\",ipyclient=mpi1)\n",
    "filter2.stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Which filter_adapters worked best?\n",
    "The primary assembly or the filter2 assembly?<br>\n",
    "use this parameter in trimed read assembly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#.set_params('filter_adapters', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trimread.run(\"2\",ipyclient=mpi1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trimread.stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Which filter trim_read worked best? \n",
    "The primary assembly or the trimread assembly? <br>\n",
    "use this parameter in the comparison of step 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#.set_params('trim_reads', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clust80.run(\"3\",ipyclient=mpi1)\n",
    "clust90.run(\"3\",ipyclient=mpi1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clust80.stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clust90.stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step four through six\n",
    "We shouldn't need to tweek the other parameters in steps 4-6. We want to stop and branch step 6 for step 7. <br> Run whichever assembly works best."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "<assembly>.run(\"456\",ipyclient=mpi1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "<assembly>.stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 7\n",
    "Now see how parameter `filter_min_trim_len` set at = 2,6 & 8 looks compareed to the primary assembly<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "<name> = <assembly_at_step6>.branch(\"<name>\") # from last assembly run 4-6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "min_sample_locus_2 = <assembly>.branch(\"min_sample_locus_2\")\n",
    "min_sample_locus_6 = <assembly>.branch(\"min_sample_locus_6\")\n",
    "min_sample_locus_8 = <assembly>.branch(\"min_sample_locus_8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "min_sample_locus_2.set_params('filter_min_trim_len', '2')\n",
    "min_sample_locus_6.set_params('filter_min_trim_len', '6')\n",
    "min_sample_locus_8.set_params('filter_min_trim_len', '8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "min_sample_locus_2.run(\"456\",ipyclient=mpi1)\n",
    "min_sample_locus_6.run(\"456\",ipyclient=mpi1)\n",
    "min_sample_locus_8.run(\"456\",ipyclient=mpi1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final Stats\n",
    "at this point I prefer the following command, the CLI stats file gives you a little more detail that is helpful, especially first set \"The number of loci caught by each filter.\"<br>\n",
    "`! cat /fs/project/adams.1970/cardenas.61/2019_analysis/<assembly name>_outfiles/<assembly name>_stats.txt`<br>\n",
    "ex: `! cat /fs/project/adams.1970/cardenas.61/2019_analysis/reduced_R1_outfiles/reduced_R1_stats.txt`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#compare final stats of all three\n",
    "! cat /fs/project/adams.1970/cardenas.61/2019_analysis_final/<assembly name>_outfiles/<assembly name>_stats.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#compare final stats of all three\n",
    "! cat /fs/project/adams.1970/cardenas.61/2019_analysis_final/<assembly name>_outfiles/<assembly name>_stats.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#compare final stats of all three\n",
    "! cat /fs/project/adams.1970/cardenas.61/2019_analysis_final/<assembly name>_outfiles/<assembly name>_stats.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Analysis\n",
    "These are example (mostly) Ipyrad analysis programs see documentation outlining [the ipyrad analysis tools]( https://ipyrad.readthedocs.io/analysis.html#ipyrad-api-analysis-tools) for cookbooks on how to generally run these analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PCA\n",
    "provided in ipyrad analysis toolkit; https://radcamp.github.io/NYC2018/04_PCA_API.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RAxML tree\n",
    "Need to test RAxML tree and see if we can find a way to partition our data... `MAGNET` shell scripts might solve this! If we can get it working...\n",
    "<br>\n",
    "<br>\n",
    "see FASconCAT-G & gphocs2multiphylip.sh script"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### tetrad-- quartet tree inference\n",
    "much like SVDquartets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Coalescent analysis?\n",
    "... biogeogrphy here though... bucky?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *structure* analysis\n",
    "[see ipyrad documentation](https://nbviewer.jupyter.org/github/dereneaton/ipyrad/blob/master/tests/cookbook-structure-pedicularis.ipynb)\n",
    "#### input and output file locations\n",
    "#### create *structure* class object\n",
    "#### set parameters for *structure* object\n",
    "#### submit job on the cluster\n",
    "#### summarize replicates with clump\n",
    "#### calculate the best K and test for convergence\n",
    "#### create structure plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Map Samples\n",
    "May use Python, but we have a pretty straightforward way of doing this in R right now. This will be based off clusters/k values, if any, and mapping those. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## See { notebook name } for further R analysis!\n",
    "1. test genetic distance by geographic distance\n",
    "2. map samples (see.... ???)\n",
    "3. check HWE (poppr & adagenet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore admixture\n",
    "using TREEMIX & ABBA-BABA admixture inference<br>\n",
    "may not be relevant!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
